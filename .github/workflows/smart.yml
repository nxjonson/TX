name: smart

on:
  schedule:
    - cron: '45 21 */2 * *'
  workflow_dispatch:

jobs:
  filter:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download and process playlists
        run: |
          # 下载第一个源文件
          curl -s -o /tmp/smart.m3u "https://raw.githubusercontent.com/judy-gotv/iptv/refs/heads/main/smart.m3u"
          # 下载第二个源文件
          curl -s -o /tmp/4gtv_list.txt "http://2099.tv12.xyz/list.txt"

          python3 <<'PY'
          import re

          # --- 配置 ---
          # 源1配置
          source1_file = "/tmp/smart.m3u"
          allowed_group1 = "GPT-台湾"

          # 源2配置
          source2_file = "/tmp/4gtv_list.txt"
          allowed_group2 = "4Gtv"

          # 输出文件
          output_file = "1.m3u"
          # --- 配置结束 ---

          def parse_m3u(file_path, allowed_group):
              """安全地解析 M3U 文件，返回指定分组的频道列表"""
              entries = []
              try:
                  with open(file_path, "r", encoding="utf-8") as f:
                      extinf_line = ""
                      for line in f:
                          line = line.strip()
                          if line.startswith("#EXTINF:"):
                              if f'group-title="{allowed_group}"' in line:
                                  extinf_line = line
                          elif extinf_line and line and not line.startswith("#"):
                              # 如果上一行是目标EXTINF，且当前行是URL，则保存
                              entries.append(f"{extinf_line}\n{line}")
                              extinf_line = "" # 重置
                          else:
                              # 如果EXTINF行不是目标，也重置
                              if not line.startswith("#EXTINF:"):
                                  extinf_line = ""
              except FileNotFoundError:
                  print(f"Warning: Source file not found at {file_path}")
              except Exception as e:
                  print(f"Error parsing {file_path}: {e}")
              return entries

          # 读取旧文件中已存在的URL，用于去重
          existing_urls = set()
          try:
              with open(output_file, "r", encoding="utf-8") as f:
                  for line in f:
                      if line.strip() and not line.startswith("#"):
                          existing_urls.add(line.strip())
          except FileNotFoundError:
              print(f"'{output_file}' not found, a new one will be created.")

          # --- 处理第一个源 ---
          print(f"--- Processing {source1_file} for group '{allowed_group1}' ---")
          new_entries_source1 = parse_m3u(source1_file, allowed_group1)
          print(f"Found {len(new_entries_source1)} entries in '{allowed_group1}'.")

          # --- 处理第二个源 ---
          print(f"--- Processing {source2_file} for group '{allowed_group2}' ---")
          new_entries_source2 = parse_m3u(source2_file, allowed_group2)
          print(f"Found {len(new_entries_source2)} entries in '{allowed_group2}'.")

          # 合并并去重
          all_new_entries = new_entries_source1 + new_entries_source2
          entries_to_write = []
          for entry in all_new_entries:
              url = entry.split('\n')[-1]
              if url not in existing_urls:
                  entries_to_write.append(entry)
                  existing_urls.add(url) # 防止本次运行中重复添加

          # --- 写入新内容 ---
          if entries_to_write:
              with open(output_file, "a", encoding="utf-8") as f_out:
                  for entry in entries_to_write:
                      f_out.write(entry + "\n")
     